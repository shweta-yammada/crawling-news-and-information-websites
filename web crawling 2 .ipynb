{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# web crawling of news & information websites with BeautifulSoup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are several packages that allow us to scrape information from webpages. One of the most common ones is BeautifulSoup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.7/site-packages (4.8.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /opt/anaconda3/lib/python3.7/site-packages (from beautifulsoup4) (1.9.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.7/site-packages (2.22.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install requests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### importing the necessary packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URL corresponding to each article is stored in the \"pagelinks\" array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the final step is to read in the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-dc2e9a405a01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# get article title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0matitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_21349 india none _4ca8e'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mthetitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;31m# get mail article page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0marticlebody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_61c55'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "\n",
    "page = requests.get(\"https://qz.com/latest\")\n",
    "\n",
    "soup = BeautifulSoup(page.content,'html.parser')\n",
    "weblinks = soup.find_all('article')\n",
    "\n",
    "pagelinks = []\n",
    "\n",
    "for link in weblinks[5:]:\n",
    "    url = link.contents[0].find_all('a')[0]\n",
    "    pagelinks.append('http://qz.com'+url.get('href'))\n",
    "    \n",
    "authorname = []\n",
    "title = []\n",
    "thearticle = []\n",
    "for link in pagelinks:\n",
    "    paragraphtext = []   # store the text for each article \n",
    "    # get url\n",
    "    url = link \n",
    "    # get page text \n",
    "    page = requests.get(url)\n",
    "    # parse with BFS\n",
    "    soup = BeautifulSoup(page.text,'html.parser')\n",
    "    # get author name\n",
    "    try:\n",
    "        abody = soup.find(class_='d3284 india').find('a')\n",
    "        aname = abody.get_text()\n",
    "    except:\n",
    "        aname = 'Anonymous'\n",
    "    # get article title \n",
    "    atitle = soup.find(class_='_21349 india none _4ca8e')\n",
    "    thetitle = atitle.get_text()\n",
    "    # get mail article page \n",
    "    articlebody = soup.find(class_='_61c55')\n",
    "    # get text \n",
    "    articletext = soup.find_all('p')[8:]\n",
    "    # print text\n",
    "    for paragraph in articletext[:-1]:\n",
    "        # get text only \n",
    "        text = paragraph.get_text()\n",
    "        paragraphtext.append(text)\n",
    "    # combine all paragraphs into an article \n",
    "    thearticle.append(paragraphtext)\n",
    "    authorname.append(aname)\n",
    "    title.append(thetitle)\n",
    "# join paragraphs to re-create the article \n",
    "myarticle = [' '.join(article) for article in thearticle]\n",
    "\n",
    "\n",
    "# save article data to file \n",
    "data = {'Title':title,\n",
    "       'Author':authorname,\n",
    "       'PageLink':pagelinks,\n",
    "       'Article' :myarticle,\n",
    "       'Date':datetime.now()}\n",
    "\n",
    "oldnews = pd.read_excel('quartz\\\\news.xlsx')\n",
    "news = pd.DataFrame(data=data)\n",
    "cols = ['title','Author','PageLink','Article','Date']\n",
    "news = news[cols]\n",
    "\n",
    "afronews = oldnews.append(news)\n",
    "afronews.drop_duplicates(subset = 'Title', keep='last',inplace=True)\n",
    "afronews.reset_index(inplace=True)\n",
    "afronews.drop(labels='index',axis=1,inplace=True)\n",
    "\n",
    "filename = 'quartz\\\\news.xlsx'\n",
    "wks_name = 'Data'\n",
    "\n",
    "writer = pd.ExcelWriter(filename)\n",
    "afronews.to_excel(writer,wks_name,index=False)\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5a9992988fdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# get article title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0matitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"_21349 africa none _4ca8e\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mthetitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;31m# get main article page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0marticlebody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_61c55'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-115b9a0be82e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0matitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_21349 India none _4ca8e'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# get main article page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "page = requests.get('https://qz.com/India/latest')\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "weblinks = soup.find_all('article')\n",
    "\n",
    "pagelinks = []\n",
    "for link in weblinks[5:]:\n",
    "    url = link.contents[0].find_all('a')[0]\n",
    "    pagelinks.append('http://qz.com' + url.get('href'))\n",
    "\n",
    "authorname = ['']\n",
    "title = ['']\n",
    "thearticle = ['']\n",
    "for link in pagelinks:\n",
    "\n",
    "    # store the text for each article\n",
    "\n",
    "    paragraphtext = []\n",
    "\n",
    "    # get url\n",
    "\n",
    "    url = link\n",
    "\n",
    "    # get page text\n",
    "\n",
    "    page = requests.get(url)\n",
    "\n",
    "    # parse with BFS\n",
    "\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    # get author name, if there's a named author\n",
    "\n",
    "    try:\n",
    "        abody = soup.find(class_='d3284 India').find('a')\n",
    "        aname = abody.get_text()\n",
    "    except:\n",
    "        aname = 'Anonymous'\n",
    "\n",
    "    # get article title\n",
    "\n",
    "    atitle = soup.find(class_='_21349 India none _4ca8e')\n",
    "    title = atitle.get_text()\n",
    "\n",
    "    # get main article page\n",
    "\n",
    "    articlebody = soup.find(class_='_61c55')\n",
    "\n",
    "    # get text\n",
    "\n",
    "    articletext = soup.find_all('p')[8:]\n",
    "\n",
    "    # print text\n",
    "\n",
    "    for paragraph in articletext[:-1]:\n",
    "\n",
    "        # get the text only\n",
    "\n",
    "        text = paragraph.get_text()\n",
    "        paragraphtext.append(text)\n",
    "\n",
    "    # combine all paragraphs into an article\n",
    "\n",
    "    thearticle.append(paragraphtext)\n",
    "    authorname.append(aname)\n",
    "    title.append(thetitle)\n",
    "\n",
    "# join paragraphs to re-create the article\n",
    "\n",
    "myarticle = [' '.join(article) for article in thearticle]\n",
    "\n",
    "# save article data to file\n",
    "\n",
    "data = {\n",
    "    'Title': title,\n",
    "    'Author': authorname,\n",
    "    'PageLink': pagelinks,\n",
    "    'Article': myarticle,\n",
    "    'Date': datetime.now(),\n",
    "    }\n",
    "\n",
    "oldnews = pd.read_excel('quartz\\\\news.xlsx')\n",
    "news = pd.DataFrame(data=data)\n",
    "cols = ['Title', 'Author', 'PageLink', 'Article', 'Date']\n",
    "news = news[cols]\n",
    "\n",
    "afronews = oldnews.append(news)\n",
    "afronews.drop_duplicates(subset='Title', keep='last', inplace=True)\n",
    "afronews.reset_index(inplace=True)\n",
    "afronews.drop(labels='index', axis=1, inplace=True)\n",
    "\n",
    "filename = 'quartz\\\\news.xlsx'\n",
    "wks_name = 'Data'\n",
    "\n",
    "writer = pd.ExcelWriter(filename)\n",
    "afronews.to_excel(writer, wks_name, index=False)\n",
    "\n",
    "writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
